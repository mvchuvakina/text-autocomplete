{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652dc485",
   "metadata": {},
   "source": [
    "# Автодополнение текство: Сравнение LSTM и Transformer моделей\n",
    "\n",
    "Проект по сравнению эффективности LSTM и Transformer архитектур для задачи генерации и автодополнения текста на основе уменьшенной версии датасета sentiment140.\n",
    "\n",
    "\n",
    "### Важное примечание о данных\n",
    "\n",
    "Датасет был сокращен вручную из-за ограничений вычислительных ресурсов:\n",
    "\n",
    "Исходный датасет sentiment140 содержал 1.6 млн твитов\n",
    "Использованная версия: ~38 тыс. примеров\n",
    "Причина: \"ядро умирало\" при обработке полного датасета\n",
    "Это повлияло на результаты, особенно на трансформерные модели, которые требуют больших объемов данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas -q\n",
    "!pip install torch -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install transformers -q\n",
    "!pip install rouge-score -q\n",
    "!pip install tqdm -q\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForCausalLM\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используем устройство:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148b1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import load_and_prepare_data\n",
    "from src.next_token_dataset import TextDataset\n",
    "from src.lstm_model import LSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39133005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример первых строк датасета:\n",
      "                                                text  num_tokens  \\\n",
      "0  switchfoot httptwitpiccom2y1zl awww thats a bu...          18   \n",
      "1  is upset that he cant update his facebook by t...          21   \n",
      "2  kenichan i dived many times for the ball manag...          18   \n",
      "3     my whole body feels itchy and like its on fire          10   \n",
      "4  nationwideclass no its not behaving at all im ...          21   \n",
      "\n",
      "                                              tokens  \n",
      "0  [switchfoot, httptwitpiccom2y1zl, awww, thats,...  \n",
      "1  [is, upset, that, he, cant, update, his, faceb...  \n",
      "2  [kenichan, i, dived, many, times, for, the, ba...  \n",
      "3  [my, whole, body, feels, itchy, and, like, its...  \n",
      "4  [nationwideclass, no, its, not, behaving, at, ...  \n",
      "Пример токенов:\n",
      "tensor([[  101,  6942, 13064,  8299,  2102,  9148, 25856,  2594,  9006,  2475,\n",
      "          2100,  2487,  2480,  2140, 22091,  2860,  2860,  2008,  2015,   102],\n",
      "        [  101,  2003,  6314,  2008,  2002,  2064,  2102, 10651,  2010,  9130,\n",
      "          2011,  3793,  2075,  2009,  1998,  2453,  5390,  2004,  1037,   102]])\n"
     ]
    }
   ],
   "source": [
    "df, tokenizer, tokens = load_and_prepare_data(data_dir=\"/Users/maria.chuvakinamail.ru/text-autocomplete/data\", max_len=20, min_len=7, use_sample=True)\n",
    "\n",
    "print(\"Пример первых строк датасета:\")\n",
    "print(df.head())\n",
    "print(\"Пример токенов:\")\n",
    "print(tokens['input_ids'][:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd933773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(30522, 128)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=30522, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "model = LSTMModel(vocab_size, embed_dim=128, hidden_dim=128, num_layers=1).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc1719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры выборок: 330 41 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_examples(tokens_ids):\n",
    "    X_list, Y_list = [], []\n",
    "    for token in tokens_ids:\n",
    "        X = token[:-1]\n",
    "        Y = token[1:]\n",
    "        X_list.append(torch.tensor(X))\n",
    "        Y_list.append(torch.tensor(Y))\n",
    "    return X_list, Y_list\n",
    "\n",
    "token_ids = tokens['input_ids'].tolist()\n",
    "train_ids, temp_ids = train_test_split(token_ids, test_size=0.2, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train, Y_train = create_examples(train_ids)\n",
    "\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Функция генерации последовательности\n",
    "def generate_sequence(model, start_seq, max_len=20):\n",
    "    model.eval()\n",
    "    generated = start_seq.tolist()\n",
    "    input_seq = start_seq.unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    for _ in range(max_len):\n",
    "        logits, hidden = model(input_seq, hidden)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        generated.append(next_token.item())\n",
    "        input_seq = torch.cat([input_seq, next_token.unsqueeze(0)], dim=1)\n",
    "    return generated\n",
    "\n",
    "# Тренировка одной эпохи\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, Y in tqdm(loader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(X)\n",
    "        loss = criterion(logits.view(-1, vocab_size), Y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Оценка ROUGE\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            for i in range(X.size(0)):\n",
    "                seq_input = X[i][:int(0.75*X.size(1))]\n",
    "                target_seq = Y[i][int(0.75*Y.size(1)):]\n",
    "                pred_tokens = generate_sequence(model, seq_input, max_len=len(target_seq))\n",
    "                pred_text = tokenizer.decode(pred_tokens[-len(target_seq):])\n",
    "                target_text = tokenizer.decode(target_seq.tolist())\n",
    "                score = scorer.score(pred_text, target_text)['rougeL'].fmeasure\n",
    "                scores.append(score)\n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "X_val, Y_val = create_examples(val_ids)\n",
    "X_test, Y_test = create_examples(test_ids)\n",
    "\n",
    "train_dataset = TextDataset(X_train, Y_train)\n",
    "val_dataset = TextDataset(X_val, Y_val)\n",
    "test_dataset = TextDataset(X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Размеры выборок:\")\n",
    "print('Размер тренирововчной выборки: ', len(train_dataset))\n",
    "print('Размер валидационной выборки: ', len(val_dataset))\n",
    "print('Размер обучающей выборки: ', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba0bcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]/Users/maria.chuvakinamail.ru/text-autocomplete/src/next_token_dataset.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.X[idx]), torch.tensor(self.Y[idx])\n",
      "100%|██████████| 21/21 [00:01<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=9.9155, ROUGE-L=0.3593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:01<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=7.1513, ROUGE-L=0.3463\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Этап 3: Тренировка LSTM\n",
    "\n",
    "for epoch in range(2):\n",
    "    loss = train_epoch(model, train_loader)\n",
    "    rouge_score = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, ROUGE-L={rouge_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643da9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "# Этап 4: Предобученный трансформер\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model_gpt = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "\n",
    "def generate_gpt2(text, max_new_tokens=20):\n",
    "    inputs = tokenizer_gpt(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model_gpt.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer_gpt.eos_token_id  # важно для GPT2\n",
    "    )\n",
    "    return tokenizer_gpt.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "# Проверка на одном примере \n",
    "text = df['text'][0]\n",
    "input_text = ' '.join(text.split()[:int(len(text.split())*0.75)])\n",
    "target_text = ' '.join(text.split()[int(len(text.split())*0.75):])\n",
    "pred_text = generate_gpt2(input_text)\n",
    "score = scorer.score(pred_text, target_text)['rougeL'].fmeasure\n",
    "\n",
    "print(\"Input:\", input_text)\n",
    "print(\"Target:\", target_text)\n",
    "print(\"Prediction:\", pred_text)\n",
    "print(\"ROUGE-L score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c81911",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for text in df['text'][:5]:  # берем только 5 примеров\n",
    "    input_text = ' '.join(text.split()[:int(len(text.split())*0.75)])\n",
    "    target_text = ' '.join(text.split()[int(len(text.split())*0.75):])\n",
    "    pred_text = generate_gpt2(input_text)\n",
    "    score = scorer.score(pred_text, target_text)['rougeL'].fmeasure\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"Average ROUGE-L for 5 examples:\", sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Этап 5: Выводы\n",
    "print(\"Сравнение моделей:\")\n",
    "print(\"- LSTM: ROUGE-L на валидации (после 2 эпох):\", rouge_score)\n",
    "print(\"- DistilGPT2: ROUGE-L на первых 50 примерах:\", sum(scores)/len(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cfe1c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e72ede56",
   "metadata": {},
   "source": [
    "Тк запускаю локально, к сожалению, из-за вычислительных ресурсов не удается проверить метрики качества, но прилагаю результаты, которые получились на ВМ (по какой-то причине ноутбук не сохранился)\n",
    "\n",
    "**Результаты обучения на ВМ:**\n",
    "\n",
    "- LSTM: 0.346\t\n",
    "- DistilGPT2: 0.045\n",
    "\n",
    "**Сравнение моделей:**\n",
    "\n",
    "LSTM:\n",
    "\n",
    "- Быстрее обучение\n",
    "- Меньше требований к памяти\n",
    "- Стабильная сходимость\n",
    "- Transformer пострадал от сокращения:\n",
    "\n",
    "\n",
    "Transformer:\n",
    "- Недостаточно данных\n",
    "- Низкое качество\n",
    "- Требует полного датасета для раскрытия потенциала\n",
    "- Ключевые выводы\n",
    "\n",
    "LSTM показала лучшие результаты на сокращенных данных:\n",
    "\n",
    "Высокое качество (ROUGE-L: 0.346 vs 0.045) Эффективность на малых данных - главное преимущество Быстрая сходимость - хорошие результаты после 2 эпох Устойчивость к ограничениям ресурсов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textauto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
