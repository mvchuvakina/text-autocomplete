# Автодополнение текство: Сравнение LSTM и Transformer моделей

Проект по сравнению эффективности LSTM и Transformer архитектур для задачи генерации и автодополнения текста на основе уменьшенной версии датасета sentiment140.

##  Важное примечание о данных

**Датасет был сокращен вручную** из-за ограничений вычислительных ресурсов:

- Исходный датасет sentiment140 содержал 1.6 млн твитов
- **Использованная версия**: ~37 тыс. примеров 
- Причина: "ядро умирало" при обработке полного датасета

Это повлияло на результаты, особенно на трансформерные модели, которые требуют больших объемов данных.

## Структура проекта

text-autocomplete/
├── data/                            
│   ├── raw_dataset.csv              
│   └── raw_dataset_sample.csv       
│   └── dataset_tokenized.pt         
├── src/                             
│   ├── data_utils.py                
│   ├── next_token_dataset.py        
│   ├── lstm_model.py                
│   ├── lstm_train.py                
│   ├── eval_lstm.py                 
│   └── eval_transformer_pipeline.py 
├── models/                          
├── solution.ipynb                   
└── requirements.txt                 




## Результаты экспериментов

### Метрики качества

| Модель | ROUGE-L | Обучение (эпохи) | Примечания |
|--------|---------|------------------|------------|
| **LSTM** | **0.346** | 2 эпохи | Стабильное качество на малых данных |
| DistilGPT2 | 0.045 | - | Требует больше данных для |

### Влияние сокращения датасета

**LSTM выиграла от сокращения:**
- Быстрее обучение
- Меньше требований к памяти  
- Стабильная сходимость

**Transformer пострадал от сокращения:**
- Недостаточно данных 
- Низкое качество 
- Требует полного датасета для раскрытия потенциала


Ключевые выводы

LSTM показала лучшие результаты на сокращенных данных:

Высокое качество (ROUGE-L: 0.346 vs 0.045)
Эффективность на малых данных - главное преимущество
Быстрая сходимость - хорошие результаты после 2 эпох
Устойчивость к ограничениям ресурсов
